{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://img.kaikeba.com/web/kkb_index/img_index_logo.png)\n",
    "# 基础课第一部分（python）第七次作业\n",
    "\n",
    "各位同学大家好！今天课上演示爬虫的原理，我们来回顾一下爬虫的思路，进行爬虫练习。\n",
    "爬虫是一个程序，这个程序可以获得网页数据。\n",
    "## 爬虫的思路\n",
    "- 1.首先确定需要爬取的网URL地址   \n",
    "[空气质量指数(http://www.tianqihoubao.com/aqi/)](http://www.tianqihoubao.com/aqi/)    \n",
    "\n",
    "\n",
    "- 2.通过HTTP/HTTPS协议来获取对应的HTML页面  \n",
    "\n",
    "\n",
    "- 3.提取HTML页面内有用的数据：\n",
    "- a. 如果是需要的数据--保存\n",
    "- b. 如果有其他URL，继续执行第二步"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 爬虫练习 \n",
    "爬虫项目整体代码：  \n",
    "[高民权_中国城市空气质量数据抓取_Github](https://github.com/fortyMiles/ChineseAirConditionCrawler)  \n",
    "【没有头绪的指令】Github中的`get_location_info.py`文件对应city_coding的生成  \n",
    "\n",
    "**处理城市编码**  \n",
    "将`<div class=\"citychk\">`copy下来，进一步处理，生成 city_coding  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting bs4\n",
      "  Downloading https://files.pythonhosted.org/packages/10/ed/7e8b97591f6f456174139ec089c769f89a94a1a4025fe967691de971f314/bs4-0.0.1.tar.gz\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\anaconda3\\lib\\site-packages (from bs4) (4.8.0)\n",
      "Requirement already satisfied: soupsieve>=1.2 in c:\\anaconda3\\lib\\site-packages (from beautifulsoup4->bs4) (1.9.3)\n",
      "Building wheels for collected packages: bs4\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-cp37-none-any.whl size=1278 sha256=0e21d51b8f09af0c5175f4c20e9cd541f7edccc1017a8d96458ba2ecedb6956b\n",
      "  Stored in directory: C:\\Users\\abbyz\\AppData\\Local\\pip\\Cache\\wheels\\a0\\b0\\b2\\4f80b9456b87abedbc0bf2d52235414c3467d8889be38dd472\n",
      "Successfully built bs4\n",
      "Installing collected packages: bs4\n",
      "Successfully installed bs4-0.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "城市数量： 367\n",
      "City Code Saved In city_coding.txt!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "def get_city_code():\n",
    "    url='http://www.tianqihoubao.com/aqi/'\n",
    "    # 发送get请求\n",
    "    response = requests.get(url)\n",
    "    response.encoding = response.apparent_encoding\n",
    "    #print(response.status_code, response.ok)\n",
    "    # 打印返回的结果\n",
    "    #print(response.encoding)\n",
    "    #print(soup.title)\n",
    "    html = response.text\n",
    "    soup=BeautifulSoup(html,\"lxml\")\n",
    "    #print(type(soup))\n",
    "    #list_citychk=soup.select(\"#content > div.citychk > dl > dd > a\")\n",
    "    #list_citychk_wbr=soup.select(\"#content > div.citychk > dl > dd > wbr > a\")\n",
    "    #list_citychk_wbr2=soup.select(\"#content > div.citychk > dl > dd > wbr > wbr > a\")\n",
    "    #list_citychk.extend(list_citychk_wbr)\n",
    "    #list_citychk.extend(list_citychk_wbr2)\n",
    "    #city_code=list(map(lambda x:(str(x).split('>')[0][14:str(x).index('.')].strip('\\r\\n'),str(x).split('>')[1].split('<')[0].strip()),list_citychk))\n",
    "    #print(citychk[0])\n",
    "    citychk=soup.select(\"#content > div.citychk\")\n",
    "    city_code_html = re.findall('href=\"/aqi/\\w*.html\">.{0,5} ', str(citychk[0]))\n",
    "    #print(city_code_html)\n",
    "    city_code=list(map(lambda x:(x.split('>')[1].strip(),x.split('>')[0][11:x.index('.')]),city_code_html))\n",
    "    #去重\n",
    "    city_code = set(city_code)\n",
    "    #print(city_code)\n",
    "    return city_code\n",
    "\n",
    "city_code=get_city_code()\n",
    "\n",
    "print('城市数量：',len(city_code))\n",
    "\n",
    "with open('./city_coding.txt', 'w') as f:\n",
    "    for item in city_code:\n",
    "#        print(item)\n",
    "        f.write('\\t'.join(item) + '\\n')\n",
    "    \n",
    "print('City Code Saved In city_coding.txt!')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**首先安装包**  \n",
    "\n",
    "``` bash\n",
    "pip install bs4\n",
    "```\n",
    "参考：[Beautiful Soup 4.2.0 文档](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)\n",
    "\n",
    "** 读取city_coding **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'安康': 'ankang', '普洱': 'puer', '鹤壁': 'hebi', '湖州': 'huzhou', '南充': 'nanchong', '保定': 'baoding', '承德': 'chengde', '宣城': 'xuancheng', '福州': 'fujianfuzhou', '临沂': 'linyi', '榆林': 'yulin', '太原': 'taiyuan', '平顶山': 'pingdingshan', '荆州': 'jingzhou', '佳木斯': 'jiamusi', '那曲': 'naqu', '滨州': 'binzhou', '防城港': 'fangchenggang', '阜阳': 'fuyang', '曲靖': 'qujing', '二连浩特': 'erlianhaote', '瓦房店': 'wafangdian', '即墨': 'jimo', '宜昌': 'yichang', '洛阳': 'lvyang', '上饶': 'shangrao', '娄底': 'loudi', '白山': 'baishan', '长治': 'changzhi', '莱芜': 'laiwu', '本溪': 'benxi', '石嘴山': 'shizuishan', '抚顺': 'fushun', '邵阳': 'shaoyang', '安阳': 'anyang', '襄阳': 'xiangyang', '云浮': 'yunfu', '郴州': 'chenzhou', '广州': 'guangzhou', '河池': 'hechi', '厦门': 'xiamen', '大兴安岭': 'daxinganling', '德宏': 'dehong', '铜川': 'tongchuan', '张家口': 'zhangjiakou', '毕节': 'bijie', '溧阳': 'liyang', '济宁': 'sdjining', '沈阳': 'shenyang', '信阳': 'xinyang', '海西': 'haixi', '十堰': 'shiyan', '昆山': 'kunshan', '常熟': 'changshu', '南宁': 'nanning', '石河子': 'shihezi', '银川': 'yinchuan', '宁波': 'ningbo', '吉安': 'jian', '威海': 'weihai', '邢台': 'xingtai', '荆门': 'jingmen', '黄冈': 'huanggang', '常德': 'changde', '文山': 'wenshan', '周口': 'zhoukou', '平度': 'pingdu', '咸阳': 'xianyang', '东营': 'dongying', '吴忠': 'wuzhong', '恩施': 'enshi', '哈尔滨': 'haerbin', '山南': 'shannan', '日照': 'rizhao', '漳州': 'zhangzhou', '阿勒泰': 'aletai', '宜春': 'jxyichun', '商丘': 'shangqiu', '嘉兴': 'jiaxing', '大理': 'dali', '开封': 'kaifeng', '常州': 'changzhou', '淮安': 'huaian', '张家港': 'zhangjiagang', '黄石': 'huangshi', '北京': 'beijing', '益阳': 'yiyang', '库尔勒': 'kuerle', '忻州': 'xinzhou', '潍坊': 'weifang', '章丘': 'zhangqiu', '崇左': 'chongzuo', '蚌埠': 'bangbu', '鹤岗': 'hegang', '延边': 'yanbian', '雅安': 'yaan', '乌海': 'wuhai', '北海': 'beihai', '海口': 'haikou', '七台河': 'qitaihe', '汉中': 'hanzhong', '拉萨': 'lasa', '陇南': 'longnan', '赣州': 'ganzhou', '松原': 'songyuan', '宿迁': 'suqian', '塔城': 'tacheng', '唐山': 'tangshan', '运城': 'sxyuncheng', '包头': 'baotou', '淮南': 'huainan', '攀枝花': 'panzhihua', '吕梁': 'lvliang', '泉州': 'quanzhou', '赤峰': 'chifeng', '鄂尔多斯': 'eerduosi', '金华': 'jinhua', '惠州': 'huizhou', '南平': 'nanping', '大同': 'datong', '白城': 'baicheng', '邯郸': 'handan', '景德镇': 'jingdezhen', '成都': 'chengdu', '池州': 'chizhou', '衡水': 'hengshui', '大连': 'dalian', '泰州': 'jstaizhou', '铜陵': 'tongling', '哈密': 'hami', '海南': 'hainan', '许昌': 'xuchang', '丽水': 'lishui', '义乌': 'yiwu', '贺州': 'hezhou', '合肥': 'hefei', '怀化': 'huaihua', '齐齐哈尔': 'qiqihaer', '大庆': 'daqing', '招远': 'sdzhaoyuan', '金坛': 'jintan', '临沧': 'lincang', '果洛': 'guolv', '博州': 'xjbozhou', '河源': 'heyuan', '辽源': 'liaoyuan', '石家庄': 'shijiazhuang', '肇庆': 'zhaoqing', '中山': 'zhongshan', '青岛': 'qingdao', '乐山': 'leshan', '胶南': 'jiaonan', '湘潭': 'xiangtan', '湘西': 'xiangxi', '贵阳': 'guiyang', '秦皇岛': 'qinhuangdao', '迪庆': 'diqing', '蓬莱': 'penglai', '绵阳': 'mianyang', '衡阳': 'hengyang', '乌鲁木齐': 'wulumuqi', '德阳': 'deyang', '临安': 'linan', '甘南': 'gannan', '自贡': 'zigong', '西双版纳': 'xishuangbanna', '临夏': 'linxia', '锦州': 'jinzhou', '连云港': 'lianyungang', '安顺': 'anshun', '玉溪': 'yuxi', '定西': 'dingxi', '宿州': 'anhuisuzhou', '宜宾': 'yibin', '杭州': 'hangzhou', '玉林': 'guangxiyulin', '桂林': 'guilin', '潮州': 'chaozhou', '龙岩': 'longyan', '绍兴': 'shaoxing', '红河': 'honghe', '凉山': 'liangshan', '聊城': 'liaocheng', '深圳': 'shenzhen', '呼伦贝尔': 'hulunbeier', '和田': 'hetian', '莱西': 'laixi', '滁州': 'chuzhou', '朝阳': 'chaoyang', '长春': 'changchun', '徐州': 'xuzhou', '九江': 'jiujiang', '西安': 'xian', '营口': 'yingkou', '广元': 'guangyuan', '通化': 'tonghua', '六安': 'liuan', '张掖': 'zhangye', '衢州': 'quzhou', '牡丹江': 'mudanjiang', '江阴': 'jiangyin', '三亚': 'sanya', '韶关': 'shaoguan', '安庆': 'anqing', '文登': 'wendeng', '南阳': 'nanyang', '汕头': 'shantou', '汕尾': 'shanwei', '南京': 'nanjing', '克拉玛依': 'kelamayi', '烟台': 'yantai', '咸宁': 'xianning', '抚州': 'fuzhou', '林芝': 'linzhi', '酒泉': 'jiuquan', '驻马店': 'zhumadian', '昌都': 'changdu', '海东': 'haidong', '太仓': 'taicang', '昭通': 'zhaotong', '嘉峪关': 'jiayuguan', '遂宁': 'scsuining', '泰安': 'taian', '贵港': 'guigang', '淄博': 'zibo', '德州': 'dezhou', '遵义': 'zunyi', '丽江': 'lijiang', '江门': 'jiangmen', '阿克苏': 'akesu', '岳阳': 'yueyang', '鞍山': 'anshan', '南昌': 'nanchang', '阜新': 'fuxin', '永州': 'yongzhou', '阿里': 'ali', '荣成': 'sdrongcheng', '柳州': 'liuzhou', '丹东': 'dandong', '晋中': 'jinzhong', '巴彦淖尔': 'bayannaoer', '晋城': 'jincheng', '莱州': 'laizhou', '宝鸡': 'baoji', '鸡西': 'jixi', '茂名': 'maoming', '巢湖': 'chaohu', '阿坝': 'aba', '阳泉': 'yangquan', '通辽': 'tongliao', '克州': 'kezhou', '泸州': 'luzhou', '金昌': 'jinchang', '海门': 'haimen', '玉树': 'yushu', '郑州': 'zhengzhou', '葫芦岛': 'huludao', '沧州': 'cangzhou', '保山': 'baoshan', '铁岭': 'tieling', '东莞': 'dongguang', '莆田': 'putian', '广安': 'guangan', '延安': 'yanan', '楚雄': 'chuxiong', '南通': 'nantong', '新余': 'xinyu', '朔州': 'shuozhou', '三门峡': 'sanmenxia', '孝感': 'xiaogan', '乳山': 'rushan', '天津': 'tianjin', '揭阳': 'jieyang', '五家渠': 'wujiaqu', '阿拉善盟': 'alashanmeng', '兴安盟': 'xinganmeng', '资阳': 'ziyang', '济南': 'jinan', '镇江': 'zhenjiang', '廊坊': 'langfang', '濮阳': 'puyang', '昆明': 'kunming', '内江': 'neijiang', '长沙': 'changsha', '鹰潭': 'yingtan', '盘锦': 'panjin', '黄山': 'huangshan', '胶州': 'jiaozhou', '达州': 'dazhou', '宁德': 'ningde', '天水': 'tianshui', '钦州': 'gxqinzhou', '白银': 'baiyin', '铜仁': 'tongren', '海北': 'haibei', '鄂州': 'ezhou', '清远': 'gdqingyuan', '锡林郭勒': 'xilinguole', '庆阳': 'gsqingyang', '平凉': 'pingliang', '吉林': 'jilin', '舟山': 'zhoushan', '漯河': 'luohe', '黔南': 'qiannan', '日喀则': 'rikaze', '苏州': 'suzhou', '宜兴': 'yixing', '马鞍山': 'maanshan', '临汾': 'linfen', '阳江': 'yangjiang', '甘孜': 'ganzi', '吴江': 'wujiang', '西宁': 'xining', '双鸭山': 'shuangyashan', '辽阳': 'liaoyang', '新乡': 'xinxiang', '株洲': 'zhuzhou', '富阳': 'zjfuyang', '寿光': 'shouguang', '佛山': 'foshan', '台州': 'taizhou', '六盘水': 'liupanshui', '亳州': 'bozhou', '盐城': 'yancheng', '绥化': 'suihua', '随州': 'suizhou', '黑河': 'heihe', '眉山': 'meishan', '四平': 'siping', '张家界': 'zhangjiajie', '黄南': 'huangnan', '无锡': 'wuxi', '吐鲁番': 'tulufan', '珠海': 'zhuhai', '枣庄': 'zaozhuang', '梧州': 'wuzhou', '黔西南': 'qianxinan', '思茅': 'simao', '怒江': 'nujiang', '巴中': 'bazhong', '来宾': 'laibin', '商洛': 'shanglv', '兰州': 'lanzhou', '梅州': 'meizhou', '芜湖': 'wuhu', '句容': 'jurong', '淮北': 'huaibei', '中卫': 'zhongwei', '昌吉': 'changji', '呼和浩特': 'huhehaote', '重庆': 'chongqing', '温州': 'wenzhou', '萍乡': 'pingxiang', '三明': 'sanming', '伊春': 'yichun', '上海': 'shanghai', '固原': 'nxguyuan', '渭南': 'weinan', '菏泽': 'heze', '武汉': 'wuhan', '扬州': 'yangzhou', '湛江': 'zhanjiang', '武威': 'wuwei', '焦作': 'jiaozuo', '喀什': 'kashi', '黔东南': 'qiandongnan'}\n"
     ]
    }
   ],
   "source": [
    "def get_city_coding(file='./city_coding.txt'):\n",
    "    city_code={}\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f.readlines():\n",
    "            item=line.strip('\\n').split('\\t')\n",
    "            city_code[item[0]]=item[1]\n",
    "    return city_code     \n",
    "city_coding =get_city_coding()\n",
    "print(city_coding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 指定城市的URL地址确定 **\n",
    "- 如何拼接成自己想要的URL地址？  \n",
    "  如果是当前月份可以看到直接使用城市名称即可，如 http://www.tianqihoubao.com/aqi/hangzhou.html  \n",
    "  如果查询的是历史月份，可以看到是这种格式 http://www.tianqihoubao.com/aqi/hangzhou-201702.html   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://www.tianqihoubao.com/aqi/guangzhou.html\n",
      "http://www.tianqihoubao.com/aqi/guangzhou.html\n",
      "http://www.tianqihoubao.com/aqi/guangzhou-201903.html\n",
      "http://www.tianqihoubao.com/aqi/guangzhou-201911.html\n"
     ]
    }
   ],
   "source": [
    "def build_url(city_code, year=None, month=None):\n",
    "    url='http://www.tianqihoubao.com/aqi/'\n",
    "    if city_code==None or city_code=='':\n",
    "        print('城市不能为空')\n",
    "        return None\n",
    "    if(year!=None and isinstance (year,int)==False):\n",
    "        print('年份只允许为整数')\n",
    "        return None\n",
    "    if(month!=None and isinstance (month,int)==False):\n",
    "        print('月份只允许为整数')\n",
    "        return None\n",
    "    \n",
    "    if year!=None and month!=None:\n",
    "        if month<10:\n",
    "            month='0'+str(month)\n",
    "        return '{url}{city_code}-{year}{month}.html'.format(url=url,city_code=city_code,year=year,month=month)\n",
    "    else:\n",
    "        return '{url}{city_code}.html'.format(url=url,city_code=city_code)\n",
    "\n",
    "city_name='广州'\n",
    "city_codes =get_city_coding()    \n",
    "city_code=city_codes[city_name]\n",
    "print(build_url(city_code))\n",
    "print(build_url(city_code,2019))\n",
    "print(build_url(city_code,2019,3))\n",
    "print(build_url(city_code,2019,11))\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** HTTP 请求状态了解 **\n",
    "- 200 - 请求成功\n",
    "- 404 - 请求的资源（网页等）不存在\n",
    "- 403 - 服务器理解请求客户端的请求，但是拒绝执行此请求     \n",
    "\n",
    "** 模拟浏览器发送http请求 **\n",
    "- get post\n",
    "\n",
    "** 获得响应的数据 分析 保存**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('城市', '监测点', 'AQI指数', '空气质量状况', 'PM10', 'PM2.5', 'Co', 'No2', 'So2', 'O3'), ('广州', '广雅中学', '39', '优', '39', '24', '0.7', '28', '7', '70'), ('广州', '市五中', '42', '优', '42', '27', '0.7', '28', '5', '68'), ('广州', '广东商学院', '45', '优', '45', '26', '0.6', '36', '9', '69'), ('广州', '市八十六中', '58', '良', '65', '34', '0.8', '63', '11', '47'), ('广州', '番禺中学', '43', '优', '43', '24', '0.6', '22', '7', '87'), ('广州', '花都师范', '45', '优', '45', '28', '0.6', '26', '6', '76'), ('广州', '市监测站', '38', '优', '38', '24', '0.7', '27', '6', '73'), ('广州', '九龙镇镇龙', '41', '优', '41', '26', '0.8', '12', '5', '78'), ('广州', '麓湖', '37', '优', '37', '23', '0.7', '29', '7', '67'), ('广州', '帽峰山森林公园', '30', '优', '30', '20', '0.6', '16', '5', '70'), ('广州', '体育西', '51', '良', '51', '31', '0.7', '34', '8', '65')]\n"
     ]
    }
   ],
   "source": [
    "def parse(url, city_name):\n",
    "    # 发送get请求\n",
    "    response = requests.get(url)\n",
    "    response.encoding = response.apparent_encoding\n",
    "    #print(response.status_code, response.ok)\n",
    "    result=[]\n",
    "    if response.status_code==200 and response.ok:\n",
    "        html_data = response.text\n",
    "        soup=BeautifulSoup(html_data,\"lxml\")\n",
    "        data_table = soup.table\n",
    "        #print(data_table)\n",
    "        #name_index = 1\n",
    "        #print(data_table.contents)\n",
    "        for index, data in enumerate(data_table.contents):\n",
    "            if data!='\\n':\n",
    "                #print(data)\n",
    "                #print(type(data))\n",
    "                #print(data.text)\n",
    "                if index == 1:\n",
    "                    result.append(tuple(['城市'] + data.text.split()))\n",
    "                else:\n",
    "                    result.append(tuple([city_name] + data.text.split())) \n",
    "        #print(result)      \n",
    "    else:\n",
    "        print('地址：{0}有误，数据获取失败'.format(url))\n",
    "    return result\n",
    "        \n",
    "city_name='广州'\n",
    "city_codes =get_city_coding()    \n",
    "city_code=city_codes[city_name]\n",
    "url=build_url(city_code)\n",
    "result=parse(url, city_name)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "def save_csv(file, data):\n",
    "    if data == None or len(data) == 1: return\n",
    "    if os.path.exists(file):\n",
    "        with open(file, 'a',newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(data[1:])\n",
    "    else:\n",
    "        with open(file, 'w',newline='') as f:\n",
    "            writer = csv.writer(f)\n",
    "            writer.writerows(data)\n",
    "            \n",
    "file = f'./data/{city_name}_2019.csv'\n",
    "save_csv(file, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 整体流程 **"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第5个城市：杭州 2018-12 26                                                                                                                                                                 "
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def crawler_all(specific_cities:list,start_year=2019,end_year=2012):\n",
    "    file = './data/allcity_2019.csv'\n",
    "    \n",
    "    allcities = list(city_codes.keys())\n",
    "   \n",
    "    #若输入了指定城市，则获取指定城市的数据，否则获取全部城市的数据\n",
    "    if specific_cities!=None and len(specific_cities)>0:\n",
    "        allcities=specific_cities\n",
    "    for index,city in enumerate(allcities):\n",
    "        city_code = city_codes[city]\n",
    "        for year in range(start_year,end_year,-1):\n",
    "            for month in range(1,13):\n",
    "                url = build_url(city_code, year, month)\n",
    "                result = parse(url, city) # city\n",
    "                msg=f'\\r第{(index+1)}个城市：{city} {year}-{month} {len(result)}'\n",
    "                print(msg+ \"  \"*(100-len(msg)), end='')\n",
    "                save_csv(file, result)\n",
    "                time.sleep(1)\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    crawler_all(['北京','上海','沈阳','大连','杭州'],end_year=2017)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
