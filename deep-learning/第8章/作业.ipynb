{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity='all'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.在fake_or_real_news数据集上构建一个简单的文本分类器， 实现对虚假新闻的分类， 可以使用其他模型和评测指标对比分类效果；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 4 columns):\n",
      "Unnamed: 0    6335 non-null int64\n",
      "title         6335 non-null object\n",
      "text          6335 non-null object\n",
      "label         6335 non-null object\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 198.1+ KB\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('fake_or_real_news.csv')\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE  \n",
       "4  It's primary day in New York and front-runners...  REAL  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建label列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6335 entries, 0 to 6334\n",
      "Data columns (total 5 columns):\n",
      "Unnamed: 0    6335 non-null int64\n",
      "title         6335 non-null object\n",
      "text          6335 non-null object\n",
      "label         6335 non-null object\n",
      "FakeOrReal    6335 non-null int64\n",
      "dtypes: int64(2), object(3)\n",
      "memory usage: 247.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "      <th>FakeOrReal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8476</td>\n",
       "      <td>You Can Smell Hillary’s Fear</td>\n",
       "      <td>Daniel Greenfield, a Shillman Journalism Fello...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10294</td>\n",
       "      <td>Watch The Exact Moment Paul Ryan Committed Pol...</td>\n",
       "      <td>Google Pinterest Digg Linkedin Reddit Stumbleu...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3608</td>\n",
       "      <td>Kerry to go to Paris in gesture of sympathy</td>\n",
       "      <td>U.S. Secretary of State John F. Kerry said Mon...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10142</td>\n",
       "      <td>Bernie supporters on Twitter erupt in anger ag...</td>\n",
       "      <td>— Kaydee King (@KaydeeKing) November 9, 2016 T...</td>\n",
       "      <td>FAKE</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>875</td>\n",
       "      <td>The Battle of New York: Why This Primary Matters</td>\n",
       "      <td>It's primary day in New York and front-runners...</td>\n",
       "      <td>REAL</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              title  \\\n",
       "0        8476                       You Can Smell Hillary’s Fear   \n",
       "1       10294  Watch The Exact Moment Paul Ryan Committed Pol...   \n",
       "2        3608        Kerry to go to Paris in gesture of sympathy   \n",
       "3       10142  Bernie supporters on Twitter erupt in anger ag...   \n",
       "4         875   The Battle of New York: Why This Primary Matters   \n",
       "\n",
       "                                                text label  FakeOrReal  \n",
       "0  Daniel Greenfield, a Shillman Journalism Fello...  FAKE           0  \n",
       "1  Google Pinterest Digg Linkedin Reddit Stumbleu...  FAKE           0  \n",
       "2  U.S. Secretary of State John F. Kerry said Mon...  REAL           1  \n",
       "3  — Kaydee King (@KaydeeKing) November 9, 2016 T...  FAKE           0  \n",
       "4  It's primary day in New York and front-runners...  REAL           1  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['FakeOrReal'] = df['label'].map(lambda x: 1 if 'REAL'==x else 0)\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 定义标签，训练集，测试集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标签\n",
    "y = df['FakeOrReal']\n",
    "\n",
    "# 创建训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(df['text'],\n",
    "                                                    y,\n",
    "                                                    test_size=0.3,\n",
    "                                                    random_state=53)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化CountVectorizer对象\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_train = count_vectorizer.fit_transform(X_train.values)\n",
    "count_test = count_vectorizer.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 使用TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# 初始化TfidfVectorizer对象\n",
    "tfidf_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.7)\n",
    "tfidf_train = tfidf_vectorizer.fit_transform(X_train.values)\n",
    "tfidf_test = tfidf_vectorizer.transform(X_test.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建CountVectorizer词向量数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000031</th>\n",
       "      <th>000035</th>\n",
       "      <th>00006</th>\n",
       "      <th>0001</th>\n",
       "      <th>0001pt</th>\n",
       "      <th>000billion</th>\n",
       "      <th>000ft</th>\n",
       "      <th>...</th>\n",
       "      <th>حلب</th>\n",
       "      <th>عربي</th>\n",
       "      <th>عن</th>\n",
       "      <th>لم</th>\n",
       "      <th>ما</th>\n",
       "      <th>محاولات</th>\n",
       "      <th>من</th>\n",
       "      <th>هذا</th>\n",
       "      <th>والمرضى</th>\n",
       "      <th>ยงade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57870 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  0000  00000031  000035  00006  0001  0001pt  000billion  000ft  \\\n",
       "0   0    0     0         0       0      0     0       0           0      0   \n",
       "1   0    3     0         0       0      0     0       0           0      0   \n",
       "2   0    1     0         0       0      0     0       0           0      0   \n",
       "3   0    1     0         0       0      0     0       0           0      0   \n",
       "4   0    0     0         0       0      0     0       0           0      0   \n",
       "\n",
       "   ...  حلب  عربي  عن  لم  ما  محاولات  من  هذا  والمرضى  ยงade  \n",
       "0  ...    0     0   0   0   0        0   0    0        0      0  \n",
       "1  ...    0     0   0   0   0        0   0    0        0      0  \n",
       "2  ...    0     0   0   0   0        0   0    0        0      0  \n",
       "3  ...    0     0   0   0   0        0   0    0        0      0  \n",
       "4  ...    0     0   0   0   0        0   0    0        0      0  \n",
       "\n",
       "[5 rows x 57870 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the CountVectorizer DataFrame: count_df\n",
    "count_df = pd.DataFrame(count_train.A, columns=count_vectorizer.get_feature_names())\n",
    "# Print the head of count_df\n",
    "count_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 创建TfidfVectorizer词向量数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>0000</th>\n",
       "      <th>00000031</th>\n",
       "      <th>000035</th>\n",
       "      <th>00006</th>\n",
       "      <th>0001</th>\n",
       "      <th>0001pt</th>\n",
       "      <th>000billion</th>\n",
       "      <th>000ft</th>\n",
       "      <th>...</th>\n",
       "      <th>حلب</th>\n",
       "      <th>عربي</th>\n",
       "      <th>عن</th>\n",
       "      <th>لم</th>\n",
       "      <th>ما</th>\n",
       "      <th>محاولات</th>\n",
       "      <th>من</th>\n",
       "      <th>هذا</th>\n",
       "      <th>والمرضى</th>\n",
       "      <th>ยงade</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.041696</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.031448</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014377</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57870 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    00       000  0000  00000031  000035  00006  0001  0001pt  000billion  \\\n",
       "0  0.0  0.000000   0.0       0.0     0.0    0.0   0.0     0.0         0.0   \n",
       "1  0.0  0.041696   0.0       0.0     0.0    0.0   0.0     0.0         0.0   \n",
       "2  0.0  0.031448   0.0       0.0     0.0    0.0   0.0     0.0         0.0   \n",
       "3  0.0  0.014377   0.0       0.0     0.0    0.0   0.0     0.0         0.0   \n",
       "4  0.0  0.000000   0.0       0.0     0.0    0.0   0.0     0.0         0.0   \n",
       "\n",
       "   000ft  ...  حلب  عربي   عن   لم   ما  محاولات   من  هذا  والمرضى  ยงade  \n",
       "0    0.0  ...  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
       "1    0.0  ...  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
       "2    0.0  ...  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
       "3    0.0  ...  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
       "4    0.0  ...  0.0   0.0  0.0  0.0  0.0      0.0  0.0  0.0      0.0    0.0  \n",
       "\n",
       "[5 rows x 57870 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the TfidfVectorizer DataFrame: tfidf_df\n",
    "tfidf_df = pd.DataFrame(tfidf_train.A, columns=tfidf_vectorizer.get_feature_names())\n",
    "# Print the head of tfidf_df\n",
    "tfidf_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于CountVectorizer词向量数据集的模型训练和模型评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.87      0.90       913\n",
      "           1       0.89      0.93      0.91       988\n",
      "\n",
      "    accuracy                           0.90      1901\n",
      "   macro avg       0.90      0.90      0.90      1901\n",
      "weighted avg       0.90      0.90      0.90      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "nb_classifier.fit(count_train, y_train)\n",
    "pred = nb_classifier.predict(count_test)\n",
    "# metrics.accuracy_score(y_test, pred)\n",
    "# metrics.roc_auc_score(y_test, pred)\n",
    "# metrics.confusion_matrix(y_test, pred, labels=[0, 1])\n",
    "print(metrics.classification_report(y_true=y_test, y_pred=pred,labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于TfidfVectorizer词向量数据集的模型训练和模型评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.97      0.74      0.84       913\n",
      "           1       0.80      0.98      0.88       988\n",
      "\n",
      "    accuracy                           0.86      1901\n",
      "   macro avg       0.88      0.86      0.86      1901\n",
      "weighted avg       0.88      0.86      0.86      1901\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb_classifier = MultinomialNB()\n",
    "\n",
    "nb_classifier.fit(tfidf_train, y_train)\n",
    "pred = nb_classifier.predict(tfidf_test)\n",
    "\n",
    "# metrics.accuracy_score(y_test, pred)\n",
    "# metrics.roc_auc_score(y_test, pred)\n",
    "# metrics.confusion_matrix(y_test, pred, labels=[0, 1])\n",
    "print(metrics.classification_report(y_true=y_test, y_pred=pred,labels=[0, 1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型效果对比"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>结论：由于fake和real的样本各占比接近一半，分布比较均匀，使用classification_report显示CountVectorizer的accuracy比TfidfVectorizer要理想</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.多分类的问题，课上所讲的影评数据集实现对其他题材电影的分类"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>解决思路：考虑尝试使用pytorch进行文本分类</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 62423 entries, 0 to 81270\n",
      "Data columns (total 4 columns):\n",
      "title             62423 non-null object\n",
      "description       62423 non-null object\n",
      "genre             62423 non-null object\n",
      "Classification    62423 non-null int64\n",
      "dtypes: int64(1), object(3)\n",
      "memory usage: 2.4+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('IMDb movies.csv')\n",
    "df = df.loc[:, ['title','description', 'genre']]\n",
    "df = df[df.description.notnull()]\n",
    "\n",
    "def getClassification(x):\n",
    "    if 'Romance' in [i.strip() for i in x.split(',')]:\n",
    "        return 1\n",
    "    elif 'Comedy' in [i.strip() for i in x.split(',')]:\n",
    "        return 2\n",
    "    elif 'Drama' in [i.strip() for i in x.split(',')]:\n",
    "        return 3\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['Classification'] = df['genre'].map(getClassification)\n",
    "# df['Classification'].value_counts()  \n",
    "df=df.drop(df[(df['Classification'].map(lambda d: d))==0].index)\n",
    "# df.head()\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification2</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>The Story of the Kelly Gang</td>\n",
       "      <td>True story of notorious Australian outlaw Ned ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Den sorte drøm</td>\n",
       "      <td>Two men of high rank are both wooing the beaut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>Cleopatra</td>\n",
       "      <td>The fabled queen of Egypt's affair with Roman ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>L'Inferno</td>\n",
       "      <td>Loosely adapted from Dante's Divine Comedy and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>From the Manger to the Cross; or, Jesus of Naz...</td>\n",
       "      <td>An account of the life of Jesus Christ, based ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Classification2                                              title  \\\n",
       "0                3                        The Story of the Kelly Gang   \n",
       "1                3                                     Den sorte drøm   \n",
       "2                3                                          Cleopatra   \n",
       "3                3                                          L'Inferno   \n",
       "4                3  From the Manger to the Cross; or, Jesus of Naz...   \n",
       "\n",
       "                                         description  \n",
       "0  True story of notorious Australian outlaw Ned ...  \n",
       "1  Two men of high rank are both wooing the beaut...  \n",
       "2  The fabled queen of Egypt's affair with Roman ...  \n",
       "3  Loosely adapted from Dante's Divine Comedy and...  \n",
       "4  An account of the life of Jesus Christ, based ...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_name=df.columns.tolist() \n",
    "col_name.insert(0,'Classification2') \n",
    "df=df.reindex(columns=col_name) \n",
    "df['Classification2']=df['Classification'] \n",
    "df=df.drop(['Classification','genre'],axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 43696 entries, 5924 to 73824\n",
      "Data columns (total 3 columns):\n",
      "Classification2    43696 non-null int64\n",
      "title              43696 non-null object\n",
      "description        43696 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 1.3+ MB\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 18727 entries, 58675 to 70269\n",
      "Data columns (total 3 columns):\n",
      "Classification2    18727 non-null int64\n",
      "title              18727 non-null object\n",
      "description        18727 non-null object\n",
      "dtypes: int64(1), object(2)\n",
      "memory usage: 585.2+ KB\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data,test_data=train_test_split(df,test_size=0.3,shuffle=True,random_state=53)\n",
    "train_data.info()\n",
    "test_data.info()\n",
    "\n",
    "train_path=\"E:/KaiKeBa/基础班/Python/第六章/第八节/homework-data/train.csv\"\n",
    "test_path=\"E:/KaiKeBa/基础班/Python/第六章/第八节/homework-data/test.csv\"\n",
    "\n",
    "train_data.to_csv(train_path,index=False,header=False) #注意导出的文件后缀要写成.csv\n",
    "test_data.to_csv(test_path,index=False,header=False)#index和header默认为True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 创建Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43696lines [00:04, 9496.00lines/s] \n",
      "43696lines [00:06, 7209.61lines/s]\n",
      "18727lines [00:02, 7444.76lines/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "from torchtext.utils import extract_archive, unicode_csv_reader\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.datasets.text_classification import *\n",
    "from torchtext.datasets.text_classification import _csv_iterator,_create_data_from_iterator\n",
    "NGRAMS = 2\n",
    "\n",
    "# 定义创建数据集函数\n",
    "def _setup_datasets(root='E:/KaiKeBa/基础班/Python/第六章/第八节/homework-data', ngrams=NGRAMS, vocab=None, include_unk=False):\n",
    "    train_csv_path = root+'/train.csv'\n",
    "    test_csv_path = root+'/test.csv'\n",
    "    if vocab is None:\n",
    "        logging.info('Building Vocab based on {}'.format(train_csv_path))\n",
    "        vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams)) #创建词典\n",
    "    else:\n",
    "        if not isinstance(vocab, Vocab):\n",
    "            raise TypeError(\"Passed vocabulary is not of type Vocab\")\n",
    "    logging.info('Vocab has {} entries'.format(len(vocab)))\n",
    "    logging.info('Creating training data')\n",
    "    train_data, train_labels = _create_data_from_iterator(   #创建训练数据\n",
    "        vocab, _csv_iterator(train_csv_path, ngrams, yield_cls=True), include_unk) \n",
    "    logging.info('Creating testing data')\n",
    "    test_data, test_labels = _create_data_from_iterator(   #创建测试数据\n",
    "        vocab, _csv_iterator(test_csv_path, ngrams, yield_cls=True), include_unk)\n",
    "    if len(train_labels ^ test_labels) > 0:\n",
    "        raise ValueError(\"Training and test labels don't match\")\n",
    "    return (TextClassificationDataset(vocab, train_data, train_labels),  #返回数据集实例\n",
    "            TextClassificationDataset(vocab, test_data, test_labels))\n",
    "\n",
    "train_dataset, test_dataset = _setup_datasets()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "VOCAB_SIZE = len(train_dataset.get_vocab())\n",
    "EMBED_DIM = 32\n",
    "NUN_CLASS = len(train_dataset.get_labels())\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUN_CLASS).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义batch生成函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def train_func(sub_train_):\n",
    "\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "\n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型验证函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def valid(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义模型测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(device), offsets.to(device), cls.to(device)\n",
    "        with torch.no_grad():\n",
    "            save_model = torch.load('E:\\KaiKeBa\\基础班\\Python\\第六章\\第八节\\model\\TextSentiment-model.pkl')\n",
    "            output = save_model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "\n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 29 seconds\n",
      "\tLoss: 0.2566(train)\t|\tAcc: 47.3%(train)\n",
      "\tLoss: 0.0009(valid)\t|\tAcc: 51.3%(valid)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\serialization.py:402: UserWarning: Couldn't retrieve source code for container of type TextSentiment. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2  | time in 0 minutes, 28 seconds\n",
      "\tLoss: 0.2321(train)\t|\tAcc: 56.3%(train)\n",
      "\tLoss: 0.0010(valid)\t|\tAcc: 55.6%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 20 seconds\n",
      "\tLoss: 0.2073(train)\t|\tAcc: 63.1%(train)\n",
      "\tLoss: 0.0013(valid)\t|\tAcc: 57.1%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 19 seconds\n",
      "\tLoss: 0.1808(train)\t|\tAcc: 68.9%(train)\n",
      "\tLoss: 0.0013(valid)\t|\tAcc: 58.7%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 19 seconds\n",
      "\tLoss: 0.1505(train)\t|\tAcc: 75.3%(train)\n",
      "\tLoss: 0.0026(valid)\t|\tAcc: 56.8%(valid)\n",
      "Epoch: 6  | time in 0 minutes, 19 seconds\n",
      "\tLoss: 0.1176(train)\t|\tAcc: 81.9%(train)\n",
      "\tLoss: 0.0015(valid)\t|\tAcc: 55.7%(valid)\n",
      "Epoch: 7  | time in 0 minutes, 19 seconds\n",
      "\tLoss: 0.0866(train)\t|\tAcc: 87.9%(train)\n",
      "\tLoss: 0.0022(valid)\t|\tAcc: 55.3%(valid)\n",
      "Epoch: 8  | time in 0 minutes, 20 seconds\n",
      "\tLoss: 0.0606(train)\t|\tAcc: 93.0%(train)\n",
      "\tLoss: 0.0030(valid)\t|\tAcc: 57.2%(valid)\n",
      "Epoch: 9  | time in 0 minutes, 19 seconds\n",
      "\tLoss: 0.0411(train)\t|\tAcc: 96.7%(train)\n",
      "\tLoss: 0.0026(valid)\t|\tAcc: 57.4%(valid)\n",
      "Epoch: 10  | time in 0 minutes, 19 seconds\n",
      "\tLoss: 0.0284(train)\t|\tAcc: 98.6%(train)\n",
      "\tLoss: 0.0034(valid)\t|\tAcc: 56.4%(valid)\n",
      "Epoch: 11  | time in 0 minutes, 20 seconds\n",
      "\tLoss: 0.0206(train)\t|\tAcc: 99.5%(train)\n",
      "\tLoss: 0.0033(valid)\t|\tAcc: 57.0%(valid)\n",
      "Epoch: 12  | time in 0 minutes, 21 seconds\n",
      "\tLoss: 0.0156(train)\t|\tAcc: 99.8%(train)\n",
      "\tLoss: 0.0028(valid)\t|\tAcc: 57.1%(valid)\n",
      "Epoch: 13  | time in 0 minutes, 21 seconds\n",
      "\tLoss: 0.0126(train)\t|\tAcc: 99.9%(train)\n",
      "\tLoss: 0.0032(valid)\t|\tAcc: 56.6%(valid)\n",
      "Epoch: 14  | time in 0 minutes, 20 seconds\n",
      "\tLoss: 0.0106(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0029(valid)\t|\tAcc: 57.1%(valid)\n",
      "Epoch: 15  | time in 0 minutes, 20 seconds\n",
      "\tLoss: 0.0092(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0030(valid)\t|\tAcc: 57.0%(valid)\n",
      "Epoch: 16  | time in 0 minutes, 20 seconds\n",
      "\tLoss: 0.0082(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0032(valid)\t|\tAcc: 57.2%(valid)\n",
      "Epoch: 17  | time in 0 minutes, 20 seconds\n",
      "\tLoss: 0.0075(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0032(valid)\t|\tAcc: 56.9%(valid)\n",
      "Epoch: 18  | time in 0 minutes, 21 seconds\n",
      "\tLoss: 0.0069(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0033(valid)\t|\tAcc: 56.8%(valid)\n",
      "Epoch: 19  | time in 0 minutes, 20 seconds\n",
      "\tLoss: 0.0064(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0032(valid)\t|\tAcc: 56.6%(valid)\n",
      "Epoch: 20  | time in 0 minutes, 19 seconds\n",
      "\tLoss: 0.0061(train)\t|\tAcc: 100.0%(train)\n",
      "\tLoss: 0.0032(valid)\t|\tAcc: 57.0%(valid)\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from torch.utils.data.dataset import random_split\n",
    "N_EPOCHS = 20\n",
    "min_valid_loss = float('inf')\n",
    "\n",
    "criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.5)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_dataset) * 0.95)\n",
    "sub_train_, sub_valid_ = random_split(train_dataset, [train_len, len(train_dataset) - train_len])\n",
    "best_acc=0\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = valid(sub_valid_)\n",
    "    \n",
    "    if best_acc==0:\n",
    "        best_acc=valid_acc\n",
    "    if best_acc>0 and best_acc<valid_acc:\n",
    "        best_acc=valid_acc\n",
    "        torch.save(model, 'E:\\KaiKeBa\\基础班\\Python\\第六章\\第八节\\model\\TextSentiment-model.pkl')\n",
    "    \n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset...\n",
      "\tLoss: 0.0001(test)\t|\tAcc: 55.3%(test)\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset...')\n",
    "test_loss, test_acc = test(test_dataset)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型应用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Drama movie\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "ag_news_label = {1 : \"Romance\",\n",
    "                 2 : \"Comedy\",\n",
    "                 3 : \"Drama\"}\n",
    "best_model = torch.load('E:\\KaiKeBa\\基础班\\Python\\第六章\\第八节\\model\\TextSentiment-model.pkl')\n",
    "        \n",
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token]\n",
    "                            for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = best_model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str=\"Michael Adler has run away from his suburban home with his little brother Dylan. Hiding out in a quiet, rural town, Michael's convinced he can make a better life for both of them. While ...\"\n",
    "vocab = train_dataset.get_vocab()\n",
    "best_model = best_model.to(\"cpu\")\n",
    "print(\"This is a %s movie\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>结论：本人尝试使用pytorch搭建模型进行文本分类，由于样本总量不足够，样本分布也不均匀，因此训练出来的模型效果不是很理想，测试集准确率只达到55%</h4>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.实际工作中我们会结合中文分词工具和课上所讲的工具库完成中文的自然语言处理任务，中文分词工作本身并不复杂，可以借助jieba完成，请同学们自行学习和掌握；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入 jieba\n",
    "import jieba\n",
    "import jieba.posseg as pseg #词性标注\n",
    "import jieba.analyse as anls #关键词提取"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache C:\\Users\\abby\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.691 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【分词】：他/ 来到/ 了/ 网易/ 杭研/ 大厦\n"
     ]
    }
   ],
   "source": [
    "seg_list = jieba.cut(\"他来到了网易杭研大厦\") #默认精确模式和启用 HMM\n",
    "print(\"【分词】：\" + \"/ \".join(seg_list)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 词性标注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "他 r\n",
      "改变 v\n",
      "了 ul\n",
      "中国 ns\n"
     ]
    }
   ],
   "source": [
    "words = pseg.cut(\"他改变了中国\")\n",
    "for word, flag in words:\n",
    "    print(\"{0} {1}\".format(word, flag))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 关键词提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"此外，公司拟对全资子公司吉林欧亚置业有限公司增资4.3亿元，增资后，吉林欧亚置业注册资本由7000万元增加到5亿元。吉林欧亚置业主要经营范围为房地产开发及百货零售等业务。目前在建吉林欧亚城市商业综合体项目。2013年，实现营业收入0万元，实现净利润-139.13万元。\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于 TF-IDF 算法进行关键词提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "欧亚 0.7300142700289363\n",
      "吉林 0.659038184373617\n",
      "置业 0.4887134522112766\n",
      "万元 0.3392722481859574\n",
      "增资 0.33582401985234045\n",
      "4.3 0.25435675538085106\n",
      "7000 0.25435675538085106\n",
      "2013 0.25435675538085106\n",
      "139.13 0.25435675538085106\n",
      "实现 0.19900979900382978\n",
      "综合体 0.19480309624702127\n",
      "经营范围 0.19389757253595744\n",
      "亿元 0.1914421623587234\n",
      "在建 0.17541884768425534\n",
      "全资 0.17180164988510638\n",
      "注册资本 0.1712441526\n",
      "百货 0.16734460041382979\n",
      "零售 0.1475057117057447\n",
      "子公司 0.14596045237787234\n",
      "营业 0.13920178509021275\n"
     ]
    }
   ],
   "source": [
    "# 通过 jieba.analyse.extract_tags 方法可以基于 TF-IDF 算法进行关键词提取，该方法共有 4 个参数：\n",
    "\n",
    "# sentence：为待提取的文本\n",
    "# topK：为返回几个 TF/IDF 权重最大的关键词，默认值为 20\n",
    "# withWeight：是否一并返回关键词权重值，默认值为 False\n",
    "# allowPOS：仅包括指定词性的词，默认值为空\n",
    "\n",
    "for x, w in anls.extract_tags(s, topK=20, withWeight=True):\n",
    "    print('%s %s' % (x, w))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 基于 TextRank 算法的关键词提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "吉林 1.0\n",
      "欧亚 0.9966893354178172\n",
      "置业 0.6434360313092776\n",
      "实现 0.5898606692859626\n",
      "收入 0.43677859947991454\n",
      "增资 0.4099900531283276\n",
      "子公司 0.35678295947672795\n",
      "城市 0.34971383667403655\n",
      "商业 0.34817220716026936\n",
      "业务 0.3092230992619838\n",
      "在建 0.3077929164033088\n",
      "营业 0.3035777049319588\n",
      "全资 0.303540981053475\n",
      "综合体 0.29580869172394825\n",
      "注册资本 0.29000519464085045\n",
      "有限公司 0.2807830798576574\n",
      "零售 0.27883620861218145\n",
      "百货 0.2781657628445476\n",
      "开发 0.2693488779295851\n",
      "经营范围 0.2642762173558316\n"
     ]
    }
   ],
   "source": [
    "# 默认过滤词性（allowPOS=('ns', 'n', 'vn', 'v')）\n",
    "for x, w in anls.textrank(s, withWeight=True):\n",
    "    print('%s %s' % (x, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
