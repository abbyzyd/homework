{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment\n",
    "目前我们可以有\n",
    "## RNN Class\n",
    "RNNs 很容易实现，接受一个$x$ vector作为输入并返回一个$y$ vector。 只不过输出的内容不仅仅与当前的输入有关，还与过去的输入是相关的。那么我们可以定义一个RNN的class，通过以下调用方式来实现一次迭代："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn = RNN()\n",
    "# y = rnn.step(x) # x is an input vector, y is the RNN's output vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每调用一次`step`， state向量 $h$ 就会被更新一次， 请同学们根据课上所讲内容，完成RNN的定义，并构建一个多层RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNBlock:\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        self.Wxh = np.random.randn(hidden_size, input_size)*0.01 # input to hidden\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "        self.Why = np.random.randn(output_size, hidden_size)*0.01 # hidden to output\n",
    "        self.bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "        self.by = np.zeros((output_size, 1)) # output bias\n",
    "        self.h = np.zeros((hidden_size,1))\n",
    "        \n",
    "    # ...\n",
    "    def step(self, x):\n",
    "        # update the hidden state\n",
    "        self.h = np.tanh(np.dot(self.Wxh, x) + np.dot(self.Whh, self.h) + self.bh)\n",
    "        # compute the output vectors\n",
    "        y = np.dot(self.Why, self.h) + self.by\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00155438]\n",
      " [-0.00186172]]\n",
      "[[-0.00454833]\n",
      " [-0.00560241]]\n",
      "[[-0.00588445]\n",
      " [-0.00749679]]\n",
      "[[-0.0073429 ]\n",
      " [-0.00937057]]\n",
      "[[-0.00879182]\n",
      " [-0.01124574]]\n",
      "[[ 6.16119728e-04]\n",
      " [-9.81538329e-05]]\n"
     ]
    }
   ],
   "source": [
    "hidden_size = 100\n",
    "x =[1,3,4,5,6,0]\n",
    "y =[0,1]\n",
    "rnn = RNNBlock(1, hidden_size, len(y))\n",
    "for i in x:\n",
    "    y=rnn.step(i)\n",
    "    print(y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结合课堂代码，自己实现一个character-level 的RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练数据准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data = [\n",
    "    (\"The dog ate the apple\".split(), [\"DET\", \"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"Everybody read that book\".split(), [\"NN\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"I like that food\".split(), [\"DET\", \"V\", \"DET\", \"NN\"]),\n",
    "    (\"The boy played the football\".split(), [\"DET\", \"NN\", \"V\", \"DET\",'NN'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_to_ix = {}\n",
    "for sent, tags in training_data:\n",
    "    for word in sent:\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'The': 0, 'dog': 1, 'ate': 2, 'the': 3, 'apple': 4, 'Everybody': 5, 'read': 6, 'that': 7, 'book': 8, 'I': 9, 'like': 10, 'food': 11, 'boy': 12, 'played': 13, 'football': 14}\n"
     ]
    }
   ],
   "source": [
    "print(word_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_ix = {\"DET\": 0, \"NN\": 1, \"V\": 2}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据转换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordToIndex(word):\n",
    "    return word in word_to_ix.keys()\n",
    "\n",
    "# Just for demonstration, turn a word into a <1 x n_words> Tensor\n",
    "def wordToTensor(word):\n",
    "    tensor = torch.zeros(1, len(word_to_ix))\n",
    "    if wordToIndex(word)==True:\n",
    "        tensor[0][word_to_ix[word]] = 1\n",
    "    return tensor\n",
    "\n",
    "# Turn a sentence into a <sentence_length x 1 x n_words>,\n",
    "# or an array of one-hot word vectors\n",
    "def sentenceToTensor(sentence):\n",
    "    tensor = torch.zeros(len(sentence), 1, len(word_to_ix))\n",
    "    for li, word in enumerate(sentence):\n",
    "        if wordToIndex(word)==True:\n",
    "            tensor[li][0][word_to_ix[word]] = 1\n",
    "    return tensor\n",
    "\n",
    "def prepare_sequence(seq, to_ix):\n",
    "    idxs = [to_ix[w] for w in seq]\n",
    "    return torch.tensor(idxs, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练次数（30次）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), 1)\n",
    "        hidden = self.i2h(combined)\n",
    "        output = self.i2o(combined)\n",
    "        output = self.softmax(output)\n",
    "        return output, hidden\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运行RNN模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[-1.0089, -1.0143, -1.2993],\n",
      "        [-0.9909, -0.9943, -1.3517],\n",
      "        [-0.9849, -0.9931, -1.3622],\n",
      "        [-0.9859, -0.9901, -1.3652],\n",
      "        [-0.9864, -0.9894, -1.3654]])\n",
      "未经过训练输出的词性： ['DET', 'DET', 'DET', 'DET', 'DET']\n",
      "正确的词性： ['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "n_hidden =6\n",
    "rnn = RNN(len(word_to_ix), n_hidden, len(tag_to_ix))\n",
    "hidden =torch.zeros(1, n_hidden)\n",
    "with torch.no_grad():\n",
    "    inputs =sentenceToTensor(training_data[0][0])\n",
    "    outputs = torch.zeros(inputs.shape[0], len(tag_to_ix))\n",
    "    for i in range(inputs.shape[0]):\n",
    "        output, hidden = rnn(inputs[0],hidden)\n",
    "        outputs[i] = output\n",
    "    print('output:',outputs)\n",
    "    predict=[]\n",
    "    for i in range(outputs.shape[0]):\n",
    "        top_n, top_i = outputs[i].topk(1)\n",
    "        predict.append([k for k, v in tag_to_ix.items() if v == top_i[0].item()][0])\n",
    "        #print('characteristic:',[k for k, v in tag_to_ix.items() if v == top_i[0].item()])\n",
    "    print('未经过训练输出的词性：',predict)\n",
    "    print('正确的词性：',training_data[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(len(word_to_ix), n_hidden, len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(rnn.parameters(), lr=0.1)\n",
    "# learning_rate = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        hidden = rnn.initHidden()\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        rnn.zero_grad()\n",
    "        \n",
    "        inputs =sentenceToTensor(sentence)\n",
    "        outputs = torch.zeros(inputs.shape[0], len(tag_to_ix))\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "        for i in range(inputs.shape[0]):\n",
    "            output, hidden = rnn(inputs[0],hidden)\n",
    "            outputs[i] = output\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "#       retain_graph=True\n",
    "        optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练后运行RNN模型输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[-0.6288, -1.2339, -1.7396],\n",
      "        [-1.1723, -0.8296, -1.3700],\n",
      "        [-1.1103, -0.8283, -1.4536],\n",
      "        [-1.1807, -0.7532, -1.5048],\n",
      "        [-1.1991, -0.7319, -1.5254]])\n",
      "训练 30 次后输出的词性： ['DET', 'NN', 'NN', 'NN', 'NN']\n",
      "正确的词性： ['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    hidden = rnn.initHidden()\n",
    "    inputs =sentenceToTensor(training_data[0][0])\n",
    "    outputs = torch.zeros(inputs.shape[0], len(tag_to_ix))\n",
    "#     print(outputs)\n",
    "    for i in range(inputs.shape[0]):\n",
    "#         print(i)\n",
    "#         print(inputs[i])\n",
    "        output, hidden = rnn(inputs[0],hidden)\n",
    "        outputs[i] = output\n",
    "        #print('output tensor',output)\n",
    "        #top_n, top_i = output.topk(1)\n",
    "#     print(outputs)\n",
    "    print('output:',outputs)\n",
    "    predict=[]\n",
    "    for i in range(outputs.shape[0]):\n",
    "        top_n, top_i = outputs[i].topk(1)\n",
    "        predict.append([k for k, v in tag_to_ix.items() if v == top_i[0].item()][0])\n",
    "        #print('characteristic:',[k for k, v in tag_to_ix.items() if v == top_i[0].item()])\n",
    "    print('训练',epochs,'次后输出的词性：',predict)\n",
    "    print('正确的词性：',training_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Class （Optional）\n",
    "自定义一个LSTM网络并进行训练， 对比simple RNN的效果"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 定义LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 未训练直接运行LSTM模型输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 6\n",
    "HIDDEN_DIM = 6\n",
    "model = LSTMTagger(EMBEDDING_DIM, HIDDEN_DIM, len(word_to_ix), len(tag_to_ix))\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[-0.8931, -1.4152, -1.0563],\n",
      "        [-0.8670, -1.3930, -1.1043],\n",
      "        [-0.9267, -1.3788, -1.0433],\n",
      "        [-1.0442, -1.3711, -0.9309],\n",
      "        [-0.9040, -1.4114, -1.0462]])\n",
      "未经过训练输出的词性： ['DET', 'DET', 'DET', 'V', 'DET']\n",
      "正确的词性： ['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    predict=[]\n",
    "    print('output:',tag_scores)\n",
    "    for i in range(tag_scores.shape[0]):\n",
    "        top_n, top_i = tag_scores[i].topk(1)\n",
    "        predict.append([k for k, v in tag_to_ix.items() if v == top_i[0].item()][0])\n",
    "        #print('characteristic:',[k for k, v in tag_to_ix.items() if v == top_i[0].item()])\n",
    "    print('未经过训练输出的词性：',predict)\n",
    "    print('正确的词性：',training_data[0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练LSTM模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):  # again, normally you would NOT do 300 epochs, it is toy data\n",
    "    for sentence, tags in training_data:\n",
    "        # Step 1. Remember that Pytorch accumulates gradients.\n",
    "        # We need to clear them out before each instance\n",
    "        model.zero_grad()\n",
    "\n",
    "        # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "        # Tensors of word indices.\n",
    "        sentence_in = prepare_sequence(sentence, word_to_ix)\n",
    "        targets = prepare_sequence(tags, tag_to_ix)\n",
    "\n",
    "        # Step 3. Run our forward pass.\n",
    "        tag_scores = model(sentence_in)\n",
    "\n",
    "        # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "        #  calling optimizer.step()\n",
    "        loss = loss_function(tag_scores, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练后运行LSTM模型输出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output: tensor([[-0.2606, -1.8586, -2.6098],\n",
      "        [-2.7619, -0.2537, -1.8269],\n",
      "        [-1.6505, -1.3407, -0.6044],\n",
      "        [-0.2897, -2.6597, -1.7063],\n",
      "        [-1.8611, -0.4811, -1.4854]])\n",
      "训练 30 次后输出的词性： ['DET', 'NN', 'V', 'DET', 'NN']\n",
      "正确的词性： ['DET', 'NN', 'V', 'DET', 'NN']\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    inputs = prepare_sequence(training_data[0][0], word_to_ix)\n",
    "    tag_scores = model(inputs)\n",
    "    predict=[]\n",
    "    print('output:',tag_scores)\n",
    "    for i in range(tag_scores.shape[0]):\n",
    "        top_n, top_i = tag_scores[i].topk(1)\n",
    "        predict.append([k for k, v in tag_to_ix.items() if v == top_i[0].item()][0])\n",
    "        #print('characteristic:',[k for k, v in tag_to_ix.items() if v == top_i[0].item()])\n",
    "    print('训练',epochs,'次后输出的词性：',predict)\n",
    "    print('正确的词性：',training_data[0][1])\n",
    "\n",
    "    # The sentence is \"the dog ate the apple\".  i,j corresponds to score for tag j\n",
    "    # for word i. The predicted tag is the maximum scoring tag.\n",
    "    # Here, we can see the predicted sequence below is 0 1 2 0 1\n",
    "    # since 0 is index of the maximum value of row 1,\n",
    "    # 1 is the index of maximum value of row 2, etc.\n",
    "    # Which is DET NOUN VERB DET NOUN, the correct sequence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结论"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样的训练次数，HIDDEN_DIM，loss_function和optimizer,LSTM的准确率比SimpleRNN的要高"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "266px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
